options(java.parameters = "-Xmx4096m") # set up heap size to 4GB
options(java.parameters = "--XX:-UseGCOverheadLimit") # disabled java gc 
options(encoding="utf-8")
# load the package rJava
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_171')
require(mallet)
require(dplyr)
require(Biostrings)
require(clv)
require(rgl)

ptm <- proc.time()
allreads <- readDNAStringSet("D:/HOCTAP/HK181/DeCuongLV/datasets/bimeta_dataset/R4.fna", "fasta")
n <- length(allreads)
# oi <- seq(from = 1, to = n, by = 2) # danh s√°ch c√°c ch·ªâ s·ªë l·∫ª
# ei <- seq(from = 2, to = n, by = 2) # danh s√°ch c√°c ch·ªâ s·ªë ch·∫µn
# short reads hay paired-end reads l√† hai read ƒë∆∞·ª£c th√†nh l·∫???p t·ª´ vi·ªác c·∫Øt hai ƒë·∫ßu
# c·ªßa m·ªôt chu·ªói DNA -> paired-end reads l√† thu·ªôc c√πng m·ªôt lo√†i.
# sreads <- paste(allreads[oi], allreads[ei], sep = "") # n·ªëi t·ª´ng c·∫∑p paired-end reads (c√πng lo√†i) l·∫°i
sreads <- allreads
proc.time() - ptm

ptm2 <- proc.time()
n = length(sreads)
l = 4
# T·∫°o c√°c k-mers doc t·ª´ c√°c read (b∆∞·ªõc n√†y ch∆∞a t·ªët v√† c√≤n t·ªën nhi·ªÅu th·ªùi gian)
docs <- lapply(sreads, function(x) 
  paste(DNAStringSet(x, start=1:(width(x)-l+1), width=l), collapse = " "))
proc.time() - ptm2

ptm3 <- proc.time()
instances <- mallet.import(id.array = as.character(1:length(docs)), 
                           text.array = unlist(docs), 
                           stoplist.file = "D:/HOCTAP/HK181/DeCuongLV/en.txt",
                           token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
proc.time() - ptm3

# create a topic trainer object
ptm4 <- proc.time()
topic.model <- MalletLDA(num.topics=3, alpha.sum = 1, beta = 0.1)
proc.time() - ptm4

# Load the documents
ptm5 <- proc.time()
topic.model$loadDocuments(instances)
proc.time() - ptm5

# Get the vocabulary, and some statistics about word frequencies
#vocabulary <- topic.model$getVocabulary()
#head(vocabulary)
#word.freqs <- mallet.word.freqs(topic.model)
#head(word.freqs)

# Optimize hyperparameters ( and ) every 20 iterations, after 50 burn-in iterations
topic.model$setAlphaOptimization(20, 50)

# train a model (specify the number of iterations)
ptm6 <- proc.time()
topic.model$train(200)
proc.time() - ptm6

# run through a few iterations where we pick the best topic for each token,
# rather than sampling from the posterior distribution
ptm7 <- proc.time()
topic.model$maximize(10)
proc.time() - ptm7

# Get the probability of topics in documents and the probability of words in topics
# By default, these functions return raw word counts. 
# Here we want probabilities, so we normalize, 
# and add ‚Äúsmoothing‚Äù so that nothing has exactly 0 probability.
ptm8 <- proc.time()
doc.topics <- mallet.doc.topics(topic.model, smoothed=TRUE, normalized=TRUE)
#topic.words <- mallet.topic.words(topic.model, smoothed=TRUE, normalized=TRUE)

# convert doc.topics to df
prob_df <- as.data.frame(doc.topics)
# save to csv
write.csv(prob_df, "prob_R4_fna_LDA_3_1_0.1.csv", row.names=FALSE)
proc.time() - ptm8

# Get the top words in topic 2 
# Notice that R indexes from 1 and Java from 0, 
# so this will be the topic that mallet called topic 1.
#mallet.top.words(topic.model, word.weights = topic.words[2,], num.top.words = 10)

# Show the first document with at least 5% tokens belonging to topic 2
#docs[doc.topics[,2] > 0.05][1]

str(doc.topics)

# LDA
ptm9 <- proc.time()
# Gom c·ª•m c√°c doc.topics (reads) d√πng k-means v√† t√???nh precision, recall, f-measure
labels <- c(as.integer(rep(1, 44405)), as.integer(rep(2, 51962))) #S1
clusters <- kmeans(doc.topics, 2)
cm <- confusion.matrix(labels, as.integer(clusters$cluster))
colMaxs <- apply(cm, 2, max)
rowMaxs <- apply(cm, 1, max)
precision <- sum(colMaxs)/sum(cm)
recall <- sum(rowMaxs)/sum(cm)
f_measure <- 2/(1/precision + 1/recall)
precision
recall
f_measure
proc.time() - ptm9


# LDA + Bimeta
ptm10 <- proc.time()
# ch·∫°y Bimeta tr√™n t·∫???p short reads S1 ƒë·ªÉ t·∫°o S1.fna.seeds.txt v√† S1.fna.groups.txt
# load S1.fna.seeds.txt v√† S1.fna.groups.txt v√† t·∫°o group
# ghi ch√∫: 
#	S1.fna.seeds.txt ch·ª©a c√°c non-overlaping read thu·ªôc c√πng group (m·ªói d√≤ng ·ª©ng v·ªõi m·ªói group)
#	S1.fna.groups.txt ch·ª©a c√°c read c√≤n l·∫°i c·ªßa m·ªói group ((m·ªói d√≤ng ·ª©ng v·ªõi m·ªói group v√† c√≥ th·ªÉ r·ªóng)
seedsFile = "D:/HOCTAP/HK181/DeCuongLV/datasets/bimeta_output/R4.fna.seeds.txt"
groupsFile = "D:/HOCTAP/HK181/DeCuongLV/datasets/bimeta_output/R4.fna.groups.txt"
groupids = lapply(readLines(groupsFile),function(x) as.integer(strsplit(x, ",")[[1]]))
seedids = lapply(readLines(seedsFile),function(x) as.integer(strsplit(x, ",")[[1]]))
groups = mapply(union, seedids, groupids)
proc.time() - ptm10

ptm11 <- proc.time()
# t√???nh c√°c groupvectors, m·ªói groupvector l√† centroid (trung b√¨nh) c·ªßa c√°c vector trong c√πng group
groupvectors <- lapply(groups, function(x) 
  if(length(x) > 1) colMeans(doc.topics[x,]) else doc.topics[x,])
proc.time() - ptm11

ptm12 <- proc.time()
# h√†m chuy·ªÉn t·ª´ vector -> data frame
vector2df <- function(data) {
  nCol <- max(vapply(data, length, 0))
  #data <- lapply(data, function(row) c(row, rep(NA, nCol-length(row))))
  data <- matrix(unlist(data), nrow=length(data), ncol=nCol, byrow=TRUE)
  data.frame(data)
}

topicsdf = vector2df(groupvectors)
# gom c·ª•m c√°c groupvector
groupclusters <- kmeans(topicsdf, 2)
proc.time() - ptm12

ptm13 <- proc.time()
# g√°n c·ª•m cho c√°c vector trong c√πng group gi·ªëng v·ªõi c·ª•m c·ªßa groupcluster
ngroups = length(groups)
nreads = length(labels)
predictedclusters <- c(as.integer(rep(-1, nreads)))
for(i in 1:ngroups) {
  predictedclusters[groups[[i]]] = groupclusters$cluster[i]
}
predictedclusters <- as.integer(predictedclusters)
proc.time() - ptm13


# T√???nh precision, recall, v√† F-measure
ptm14 <- proc.time()
cm <- confusion.matrix(labels, predictedclusters)
colMaxs <- apply(cm, 2, max)
rowMaxs <- apply(cm, 1, max)
precision <- sum(colMaxs)/sum(cm)
recall <- sum(rowMaxs)/sum(cm)
f_measure <- 2/(1/precision + 1/recall)
precision
recall
f_measure
proc.time() - ptm14

